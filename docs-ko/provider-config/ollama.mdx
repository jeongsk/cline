---
title: "Ollama"
---

Cline은 Ollama를 사용하여 로컬에서 모델을 실행하는 것을 지원합니다. 이 접근 방식은 개인 정보 보호, 오프라인 액세스 및 잠재적으로 비용 절감을 제공합니다. 초기 설정과 충분히 강력한 컴퓨터가 필요합니다. 현재 소비자 하드웨어 상태 때문에 평균적인 하드웨어 구성에서는 성능이 저하될 가능성이 높으므로 Cline과 함께 Ollama를 사용하는 것은 권장되지 않습니다.

**웹사이트:** [https://ollama.com/](https://ollama.com/)

### Ollama 설정

1.  **Ollama 다운로드 및 설치:**
    [Ollama 웹사이트](https://ollama.com/)에서 운영 체제용 Ollama 설치 프로그램을 받고 설치 가이드를 따릅니다. Ollama가 실행 중인지 확인합니다. 일반적으로 다음으로 시작할 수 있습니다.

    ```bash
    ollama serve
    ```

2.  **모델 다운로드:**
    Ollama는 다양한 모델을 지원합니다. 사용 가능한 모델 목록은 [Ollama 모델 라이브러리](https://ollama.com/library)에서 찾을 수 있습니다. 코딩 작업에 권장되는 일부 모델은 다음과 같습니다.

    -   `codellama:7b-code` (좋은, 더 작은 시작점)
    -   `codellama:13b-code` (더 나은 품질, 더 큰 크기 제공)
    -   `codellama:34b-code` (훨씬 더 높은 품질 제공, 매우 큼)
    -   `qwen2.5-coder:32b`
    -   `mistralai/Mistral-7B-Instruct-v0.1` (견고한 범용 모델)
    -   `deepseek-coder:6.7b-base` (코딩에 효과적)
    -   `llama3:8b-instruct-q5_1` (일반 작업에 적합)

    모델을 다운로드하려면 터미널을 열고 다음을 실행합니다.

    ```bash
    ollama pull <모델_이름>
    ```

    예를 들어:

    ```bash
    ollama pull qwen2.5-coder:32b
    ```

3.  **모델의 컨텍스트 창 구성:**
    기본적으로 Ollama 모델은 종종 2048 토큰의 컨텍스트 창을 사용하며 이는 많은 Cline 요청에 충분하지 않을 수 있습니다. 괜찮은 결과를 얻으려면 최소 12,000 토큰이 권장되며 32,000 토큰이 이상적입니다. 이를 조정하려면 모델의 매개변수를 수정하고 새 버전으로 저장해야 합니다.

    먼저 모델을 로드합니다(예시로 `qwen2.5-coder:32b` 사용).

    ```bash
    ollama run qwen2.5-coder:32b
    ```

    Ollama 대화형 세션 내에서 모델이 로드되면 컨텍스트 크기 매개변수를 설정합니다.

    ```
    /set parameter num_ctx 32768
    ```

    그런 다음 이 구성된 모델을 새 이름으로 저장합니다.

    ```
    /save your_custom_model_name
    ```

    (`your_custom_model_name`을 원하는 이름으로 바꿉니다.)

4.  **Cline 구성:**
    -   Cline 사이드바(일반적으로 Cline 아이콘으로 표시됨)를 엽니다.
    -   설정 기어 아이콘(⚙️)을 클릭합니다.
    -   API 공급자로 "ollama"를 선택합니다.
    -   이전 단계에서 저장한 모델 이름(예: `your_custom_model_name`)을 입력합니다.
    -   (선택 사항) Ollama가 다른 컴퓨터나 포트에서 실행 중인 경우 기본 URL을 조정합니다. 기본값은 `http://localhost:11434`입니다.
    -   (선택 사항) Cline의 고급 설정에서 모델 컨텍스트 크기를 구성합니다. 이렇게 하면 Cline이 사용자 지정된 Ollama 모델로 컨텍스트 창을 효과적으로 관리하는 데 도움이 됩니다.

### 팁 및 참고 사항

-   **리소스 요구 사항:** 로컬에서 대규모 언어 모델을 실행하면 시스템 리소스에 많은 부담을 줄 수 있습니다. 컴퓨터가 선택한 모델의 요구 사항을 충족하는지 확인하십시오.
-   **모델 선택:** 다양한 모델을 실험하여 특정 작업과 기본 설정에 가장 적합한 모델을 찾으십시오.
-   **오프라인 기능:** 모델을 다운로드한 후에는 인터넷 연결 없이도 해당 모델과 함께 Cline을 사용할 수 있습니다.
-   **토큰 사용량 추적:** Cline은 Ollama를 통해 액세스하는 모델의 토큰 사용량을 추적하여 소비량을 모니터링할 수 있도록 합니다.
-   **Ollama 자체 설명서:** 자세한 내용은 공식 [Ollama 설명서](https://ollama.com/docs)를 참조하십시오.
