---
title: "Ollama"
description: "Cline과 함께 로컬 AI 모델 실행을 위한 Ollama 설정 빠른 가이드입니다."
---

### 📋 전제 조건

-   Windows, macOS 또는 Linux 컴퓨터
-   VS Code에 Cline 설치됨

### 🚀 설정 단계

#### 1. Ollama 설치

-   [ollama.com](https://ollama.com) 방문
-   운영 체제에 맞게 다운로드 및 설치

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(2)%20(1)%20(1).png"
		alt="Ollama 다운로드 페이지"
	/>
</Frame>

#### 2. 모델 선택 및 다운로드

-   [ollama.com/search](https://ollama.com/search)에서 모델 찾아보기
-   모델 선택 및 명령어 복사:

    ```bash
    ollama run [모델-이름]
    ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif"
		alt="Ollama에서 모델 선택 중"
	/>
</Frame>

-   터미널을 열고 명령어를 실행합니다.

    -   예시:

        ```bash
        ollama run llama2
        ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/starting-ollama-terminal%20(2).gif"
		alt="터미널에서 Ollama 실행 중"
	/>
</Frame>

**✨ 이제 Cline 내에서 모델을 사용할 준비가 되었습니다!**

#### 3. Cline 구성

1. VS Code 열기
2. Cline 설정 아이콘 클릭
3. API 공급자로 "Ollama" 선택
4. 구성 입력:
    - 기본 URL: `http://localhost:11434/` (기본값, 그대로 둘 수 있음)
    - 사용 가능한 옵션에서 모델 선택

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/selecting-ollama-model-cline%20(3).gif"
		alt="Ollama로 Cline 구성 중"
	/>
</Frame>

### ⚠️ 중요 참고 사항

-   Cline과 함께 사용하기 전에 Ollama를 시작합니다.
-   Ollama를 백그라운드에서 실행 상태로 유지합니다.
-   첫 번째 모델 다운로드는 몇 분 정도 걸릴 수 있습니다.

### 🔧 문제 해결

Cline이 Ollama에 연결할 수 없는 경우:

1. Ollama가 실행 중인지 확인합니다.
2. 기본 URL이 올바른지 확인합니다.
3. 모델이 다운로드되었는지 확인합니다.

더 많은 정보가 필요하십니까? [Ollama 문서](https://github.com/ollama/ollama/blob/main/docs/api.md)를 읽어보십시오.
