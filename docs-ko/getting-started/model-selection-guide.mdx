---
title: "모델 선택 가이드"
description: "최종 업데이트: 2025년 2월 5일."
---

## 컨텍스트 창 이해하기

컨텍스트 창은 컴퓨터의 RAM과 유사하게 AI 어시스턴트의 작업 메모리라고 생각하면 됩니다. 대화 중에 모델이 한 번에 "기억"하고 처리할 수 있는 정보의 양을 결정합니다. 여기에는 다음이 포함됩니다.

-   코드 파일 및 대화
-   어시스턴트의 응답
-   제공된 모든 문서 또는 추가 컨텍스트

컨텍스트 창은 토큰(영어 단어의 약 3/4)으로 측정됩니다. 모델마다 컨텍스트 창 크기가 다릅니다.

-   Claude 3.5 Sonnet: 200K 토큰
-   DeepSeek 모델: 128K 토큰
-   Gemini Flash 2.0: 1M 토큰
-   Gemini 1.5 Pro: 2M 토큰

컨텍스트 창의 한계에 도달하면 새 프로그램을 실행하기 위해 RAM을 비우는 것처럼 새 정보를 위한 공간을 만들기 위해 이전 정보를 제거해야 합니다. 이것이 때때로 AI 어시스턴트가 대화의 이전 부분을 "잊어버리는" 것처럼 보이는 이유입니다.

Cline은 다음을 보여주는 컨텍스트 창 진행률 표시줄을 통해 이 제한 사항을 관리하는 데 도움을 줍니다.

-   입력 토큰(모델로 보낸 내용)
-   출력 토큰(모델이 생성한 내용)
-   사용한 컨텍스트 창의 양을 시각적으로 표현
-   선택한 모델의 총 용량

<Frame caption="Cline의 컨텍스트 창 사용량 시각적 표현">
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(11).png"
		alt="컨텍스트 창 진행률 표시줄 예시"
	/>
</Frame>

이 가시성은 새로 시작하거나 작업을 더 작은 단위로 나누어야 할 시기를 알려줌으로써 Cline을 더 효과적으로 사용하는 데 도움이 됩니다.

### 모델 비교

## Cline용 LLM 모델 비교 (2025년 2월)

| 모델              | 입력 비용* | 출력 비용* | 컨텍스트 창 | 최적 용도                           |
| ----------------- | ------------ | ------------- | -------------- | ----------------------------------- |
| Claude 3.5 Sonnet | $3.00        | $15.00        | 200K           | 최상의 코드 구현 및 도구 사용        |
| DeepSeek R1       | $0.55        | $2.19         | 128K           | 계획 및 추론 챔피언                |
| DeepSeek V3       | $0.14        | $0.28         | 128K           | 가치 있는 코드 구현                 |
| o3-mini           | $1.10        | $4.40         | 200K           | 유연한 사용, 강력한 계획            |
| Gemini Flash 2.0  | $0.00        | $0.00         | 1M             | 강력한 만능형                      |
| Gemini 1.5 Pro    | $0.00        | $0.00         | 2M             | 대규모 컨텍스트 처리                |

*백만 토큰당 비용

### 2025년 최고의 선택

1.  **Claude 3.5 Sonnet**
    -   최고의 전체 코드 구현
    -   가장 신뢰할 수 있는 도구 사용
    -   비싸지만 중요한 코드에는 그만한 가치가 있음
2.  **DeepSeek R1**
    -   뛰어난 계획 및 추론 능력
    -   훌륭한 가치 가격 책정
3.  **o3-mini**
    -   조정 가능한 추론으로 계획에 강력함
    -   다양한 요구에 맞는 세 가지 추론 모드
    -   OpenAI Tier 3 API 액세스 필요
    -   200K 컨텍스트 창
4.  **DeepSeek V3**
    -   신뢰할 수 있는 코드 구현
    -   일상적인 코딩에 적합
    -   구현에 비용 효율적
5.  **Gemini Flash 2.0**
    -   방대한 1M 컨텍스트 창
    -   향상된 속도 및 성능
    -   전반적으로 우수한 기능

### 모드별 최고의 모델 (계획 또는 실행)

#### 계획

1.  **DeepSeek R1**
    -   동급 최고의 추론 능력
    -   복잡한 작업을 세분화하는 데 탁월함
    -   강력한 수학/알고리즘 계획
    -   MoE 아키텍처가 추론에 도움을 줌
2.  **o3-mini (높은 추론)**
    -   세 가지 추론 수준:
        -   높음: 복잡한 계획
        -   중간: 일상적인 작업
        -   낮음: 빠른 아이디어
    -   200K 컨텍스트는 대규모 프로젝트에 도움이 됨
3.  **Gemini Flash 2.0**
    -   복잡한 계획을 위한 방대한 컨텍스트 창
    -   강력한 추론 능력
    -   다단계 작업에 능숙함

#### 실행 (코딩)

1.  **Claude 3.5 Sonnet**
    -   최고의 코드 품질
    -   Cline 도구와 가장 신뢰할 수 있음
    -   중요한 코드에는 프리미엄 가치가 있음
2.  **DeepSeek V3**
    -   거의 Sonnet 수준의 코드 품질
    -   R1보다 나은 API 안정성
    -   일상적인 코딩에 적합
    -   강력한 도구 사용
3.  **Gemini 1.5 Pro**
    -   2M 컨텍스트 창
    -   복잡한 코드베이스에 능숙함
    -   신뢰할 수 있는 API
    -   강력한 다중 파일 이해

### 로컬 모델에 대한 참고 사항

비용 절감을 위해 로컬에서 모델을 실행하는 것이 매력적으로 보일 수 있지만 현재 Cline과 함께 사용할 로컬 모델은 권장하지 않습니다. [로컬 모델은 Cline의 필수 도구를 사용하는 데 훨씬 덜 신뢰할 수 있으며](https://docs.cline.bot/running-models-locally/read-me-first) 일반적으로 원래 모델 기능의 1~26%만 유지합니다. 예를 들어 DeepSeek-R1의 전체 클라우드 버전은 671B 매개변수입니다. 로컬 버전은 복잡한 작업과 도구 사용에 어려움을 겪는 대폭 단순화된 복사본입니다. 고급 하드웨어(RTX 3070+ 이상, 32GB+ RAM)를 사용하더라도 응답 속도가 느리고 도구 실행이 덜 안정적이며 기능이 저하됩니다. 최상의 개발 환경을 위해서는 위에 나열된 클라우드 모델을 사용하는 것이 좋습니다.

### 주요 시사점

1.  **계획 대 실행 중요**: 작업 유형에 따라 모델 선택
2.  **벤치마크보다 실제 성능**: 실제 Cline 성능에 집중
3.  **혼합 및 일치**: 계획 및 구현에 다른 모델 사용
4.  **비용 대 품질**: 중요한 코드에는 프리미엄 모델이 가치가 있음
5.  **백업 유지**: API 문제에 대비하여 대안 준비

_\*참고: 벤치마크뿐만 아니라 실제 사용 패턴 및 커뮤니티 피드백을 기반으로 합니다. 경험은 다를 수 있습니다. 이것은 Cline 내에서 사용할 수 있는 모든 모델의 전체 목록이 아닙니다._
